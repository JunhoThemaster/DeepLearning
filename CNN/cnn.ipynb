{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36573f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c50d85ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root = './mnist_data', train = True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root = './mnist_data', train = False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "872f10fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "sample_idx = random.randint(0, len(train_dataset)-1)\n",
    "\n",
    "image, label = train_dataset[sample_idx]\n",
    "print(image.shape)\n",
    "print(label)\n",
    "\n",
    "plt.imshow(image.squeeze(), cmap = 'gray')\n",
    "plt.title(f'Label : {label}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71efdb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle= True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cdaedf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.linear_1 = nn.Linear(input_dim, 256)\n",
    "        self.linear_2 = nn.Linear(256, 128)\n",
    "        self.linear_3 = nn.Linear(128, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)  \n",
    "        self.output = nn.Linear(64, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.linear_1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.linear_2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.linear_3(x))\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0bcf8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "image_size = 64 * 14 * 14 \n",
    "target_cnt = 10\n",
    "model = MLP(image_size, target_cnt).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c00dc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x784 and 12544x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m      9\u001b[39m     data, target = data.to(device), target.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     optimizer.zero_grad()\n\u001b[32m     13\u001b[39m     loss = criterion(pred, target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\main\\Documents\\DeepLearning\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\main\\Documents\\DeepLearning\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     17\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.flatten(x)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlinear_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     19\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.dropout(x)\n\u001b[32m     20\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.relu(\u001b[38;5;28mself\u001b[39m.linear_2(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\main\\Documents\\DeepLearning\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\main\\Documents\\DeepLearning\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\main\\Documents\\DeepLearning\\torch\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (64x784 and 12544x256)"
     ]
    }
   ],
   "source": [
    "num_epoch = 5\n",
    "patience = 3\n",
    "cnt = 0\n",
    "best_acc = 0\n",
    "for epoch in range(num_epoch+1):\n",
    "    model.train()\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        pred = model(data)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_data, test_target in test_loader:\n",
    "            test_data, test_target = test_data.to(device), test_target.to(device)\n",
    "            test_pred = model(test_data)\n",
    "            pred_target = torch.argmax(test_pred, axis = 1)\n",
    "            acc = (pred_target == test_target).float().mean()\n",
    "\n",
    "\n",
    "            \n",
    "        if acc > best_acc :\n",
    "            best_acc = acc\n",
    "            cnt = 0\n",
    "            torch.save(model.state_dict(),\"best_model.pt\")\n",
    "        else :\n",
    "            cnt += 1\n",
    "        if cnt >= patience:\n",
    "            print('조기종료')    \n",
    "            break;\n",
    "    print(f'{epoch:4d}/{num_epoch} Epoch | Test Accuracy : {acc * 100:.2f}%')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
