{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebf885e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA ì§€ì› ì—¬ë¶€: True\n",
      "PyTorch ë‚´ë¶€ CUDA ë²„ì „: 12.1\n",
      "ì»´íŒŒì¼ëœ ë²„ì „: PyTorch built with:\n",
      "  - C++ Version: 201703\n",
      "  - MSVC 192930151\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v3.3.2 (Git Hash 2dc95a2ad0841e29db8b22fbccaf3e5da7992b01)\n",
      "  - OpenMP 2019\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 12.1\n",
      "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
      "  - CuDNN 8.8.1  (built against CUDA 12.0)\n",
      "  - Magma 2.5.4\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.8.1, CXX_COMPILER=C:/cb/pytorch_1000000000000/work/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /Zc:__cplusplus /bigobj /FS /utf-8 -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE /wd4624 /wd4068 /wd4067 /wd4267 /wd4661 /wd4717 /wd4244 /wd4804 /wd4273, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.2.2, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA ì§€ì› ì—¬ë¶€:\", torch.cuda.is_available())  # ðŸ”¥ ë°˜ë“œì‹œ Trueì—¬ì•¼ GPU í•™ìŠµ ê°€ëŠ¥\n",
    "print(\"PyTorch ë‚´ë¶€ CUDA ë²„ì „:\", torch.version.cuda)\n",
    "print(\"ì»´íŒŒì¼ëœ ë²„ì „:\", torch.__config__.show())  # ë‚´ë¶€ ì»´íŒŒì¼ ì„¤ì • ë³´ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cbf2e9",
   "metadata": {},
   "source": [
    "## ì´ì§„ ë¶„ë¥˜ + ë°°ì¹˜ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "910e1a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "(569, 30) (569,)\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "df = pd.read_csv('data/breast_cancer.csv',index_col = 0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "df.head()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = df.drop('target',axis=1).values\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "y  = le.fit_transform(df['target'])\n",
    "\n",
    "# y = torch.tensor()\n",
    "\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "print(np.unique(y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00e28ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([569, 30]) torch.Size([569, 1])\n"
     ]
    }
   ],
   "source": [
    "X_tensor = torch.tensor(X,dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y,dtype=torch.float32).view(-1,1)\n",
    "\n",
    "print(X_tensor.shape,y_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af7cd61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_tensor,y_tensor,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "816026c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(X_train,y_train)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70e5a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "\n",
    "        def __init__(self,input_dim):\n",
    "\n",
    "            super().__init__()\n",
    "            self.linear1 = nn.Linear(input_dim,64)\n",
    "            self.linear2 = nn.Linear(64,32)\n",
    "            self.linear3 = nn.Linear(32,10)\n",
    "            self.output = nn.Linear(10,1)\n",
    "            self.relu = nn.ReLU()\n",
    "\n",
    "        def forward(self,x):\n",
    "\n",
    "            x = self.relu(self.linear1(x))\n",
    "            x = self.relu(self.linear2(x))\n",
    "            x = self.relu(self.linear3(x))\n",
    "            x = (self.output(x))\n",
    "            return torch.sigmoid(x)\n",
    "        \n",
    "\n",
    "model = BinaryClassifier(input_dim=X_train.shape[-1]).to(device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14f77ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.BCELoss()\n",
    "# nn.BCEWithLogitsLoss() -> sigmoid ê°€ í¬í•¨ë¨ / sigmoid + BCELoss  \n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be573ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/ 3000 | test_acc 75.44\n",
      "100/ 3000 | test_acc 93.86\n",
      "200/ 3000 | test_acc 92.11\n",
      "300/ 3000 | test_acc 87.72\n",
      "400/ 3000 | test_acc 93.86\n",
      "500/ 3000 | test_acc 95.61\n",
      "600/ 3000 | test_acc 92.98\n",
      "700/ 3000 | test_acc 95.61\n",
      "800/ 3000 | test_acc 95.61\n",
      "900/ 3000 | test_acc 95.61\n",
      "1000/ 3000 | test_acc 92.98\n",
      "1100/ 3000 | test_acc 92.98\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m loss = criterion(pred,batch_y)\n\u001b[32m     12\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m total_loss += loss.item()\n\u001b[32m     16\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\main\\miniforge3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    514\u001b[39m         Tensor.backward,\n\u001b[32m    515\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    520\u001b[39m         inputs=inputs,\n\u001b[32m    521\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\main\\miniforge3\\Lib\\site-packages\\torch\\autograd\\__init__.py:259\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    250\u001b[39m inputs = (\n\u001b[32m    251\u001b[39m     (inputs,)\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch.Tensor, graph.GradientEdge))\n\u001b[32m   (...)\u001b[39m\u001b[32m    255\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[32m    256\u001b[39m )\n\u001b[32m    258\u001b[39m grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m grad_tensors_ = \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    261\u001b[39m     retain_graph = create_graph\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\main\\miniforge3\\Lib\\site-packages\\torch\\autograd\\__init__.py:142\u001b[39m, in \u001b[36m_make_grads\u001b[39m\u001b[34m(outputs, grads, is_grads_batched)\u001b[39m\n\u001b[32m    136\u001b[39m         msg = (\n\u001b[32m    137\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    138\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    139\u001b[39m         )\n\u001b[32m    140\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[32m    141\u001b[39m     new_grads.append(\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m         \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     )\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    145\u001b[39m     new_grads.append(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "nb_epoch = 3000\n",
    "\n",
    "for epoch in range(nb_epoch):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0 \n",
    "\n",
    "    for batch_x,batch_y in train_loader:\n",
    "        batch_x,batch_y = batch_x.to(device) ,batch_y.to(device)\n",
    "        pred = model(batch_x)\n",
    "        loss = criterion(pred,batch_y)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0 :\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_pred = model(X_test.to(device))\n",
    "            # test_loss = criterion(test_pred,y_test)\n",
    "            acc = ((test_pred >= 0.5).float() == y_test.to(device)).float().mean()\n",
    "            print(f'{epoch}/ {nb_epoch} | test_acc {acc.item() * 100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcb82e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9474)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì •í™•ë„ ì²˜ëŸ¼ í™œìš© ê°€ëŠ¥\n",
    "\n",
    "((test_pred > 0.4).float() == y_test).float().mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch GPU (torch_py310)",
   "language": "python",
   "name": "torch_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
