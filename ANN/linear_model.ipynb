{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee49c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\main\\miniforge3\\Lib\\runpy.py\", line 198, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\runpy.py\", line 88, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\asyncio\\base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\asyncio\\base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3100, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3155, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3367, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3612, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3672, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\main\\AppData\\Local\\Temp\\ipykernel_5480\\311122021.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"c:\\Users\\main\\miniforge3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\main\\miniforge3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.1543, -0.6774, -1.8365,  1.2755]),\n",
       " tensor([[-0.1306,  0.2080,  2.0686],\n",
       "         [ 0.2518,  0.4478,  0.1398],\n",
       "         [ 0.4845, -0.8244,  1.2540],\n",
       "         [-0.3026,  0.9284, -0.5702]]),\n",
       " tensor([-0.1192, -1.3794,  1.1216]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "x = torch.randn(4, dtype=torch.float32)     \n",
    "W = torch.randn(4, 3, dtype=torch.float32)    \n",
    "b = torch.randn(3, dtype=torch.float32)      \n",
    "\n",
    "x,W,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18fad76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.7164,  1.2556,  0.3843])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear Layer 함수를 정의한다면,\n",
    "def linearfunction(x, W, b):\n",
    "\ty = torch.matmul(x, W) + b\n",
    "\treturn y\n",
    "\n",
    "linearfunction(x, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "090eb2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.1517, 0.3565, 0.9259]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    #생성자\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.W = torch.randn(input_dim,output_dim, dtype=torch.float32)\n",
    "        self.B = torch.randn(output_dim,dtype=torch.float32)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        y = torch.matmul(x,self.W) + self.B\n",
    "        return y\n",
    "    \n",
    "\n",
    "x = torch.randn(4, dtype=torch.float32 )\n",
    "# 생성자\n",
    "mylinear = NeuralNetwork(4, 3)\n",
    "\n",
    "y = mylinear.forward(x)\n",
    "print (y, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab9dff84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.0765, -0.5199, -2.2269], grad_fn=<AddBackward0>) torch.Size([3])\n",
      "Parameter containing:\n",
      "tensor([[ 1.2597,  0.6009, -0.6795],\n",
      "        [-0.7814,  1.2406,  0.9106],\n",
      "        [ 0.2336,  0.1506, -1.2506],\n",
      "        [ 0.6216,  0.0127,  1.7036]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.4397,  0.4934, -0.4347], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    #생성자\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.W = nn.Parameter(torch.randn(input_dim,output_dim,dtype=torch.float32))\n",
    "        self.B = nn.Parameter(torch.randn(output_dim,dtype=torch.float32))\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        y = torch.matmul(x,self.W) + self.B\n",
    "        return y\n",
    "    \n",
    "\n",
    "x = torch.randn(4, dtype=torch.float32 )\n",
    "# 생성자\n",
    "mylinear = NeuralNetwork(4, 3)\n",
    "\n",
    "y = mylinear.forward(x)\n",
    "print (y, y.shape)\n",
    "\n",
    "\n",
    "for param in mylinear.parameters():\n",
    "\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ae9e435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3829,  0.1026,  0.2650, -0.0023],\n",
      "        [-0.2323, -0.2994,  0.4680, -0.1847],\n",
      "        [ 0.2625, -0.2738,  0.0640,  0.4239]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2499,  0.2070, -0.3670], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_dim = 4\n",
    "output_dim = 3\n",
    "\n",
    "x = torch.randn(4, dtype=torch.float32 )\n",
    "\n",
    "model = nn.Linear(input_dim,output_dim)\n",
    "\n",
    "for param in model.parameters():\n",
    "\n",
    "    print(param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0133b97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9969,  0.1767,  0.0540,  0.3437, -0.7113, -0.2285, -0.4730,  0.6280,\n",
      "         0.9315, -0.5201, -0.0939, -0.7458,  0.0584, -0.0818,  0.3439, -0.2926,\n",
      "        -0.8927, -0.4619, -0.4778,  0.0723, -0.0411,  0.7116, -0.3477,  0.3604,\n",
      "         0.3762,  0.3200, -0.3912,  0.9220,  0.0786, -0.6052, -0.3709,  0.5194,\n",
      "         0.0358, -0.6950,  0.0873,  0.4288, -0.3772, -0.0640,  0.3191, -0.0865,\n",
      "        -0.6911,  0.1942,  0.3578,  0.5261, -0.2241, -0.2951, -0.3083, -0.4318,\n",
      "         0.2760, -0.5543,  0.7801,  0.2212,  0.6707, -0.4953, -0.9572, -0.6528,\n",
      "        -0.5235,  0.4494,  0.6020,  0.8165, -0.0022, -0.2851,  0.0288, -0.4650,\n",
      "        -0.7384, -0.2236, -0.2420,  0.6790,  0.3093,  1.2894,  0.6236, -0.1122,\n",
      "        -0.3746, -0.4033,  0.7868,  0.3509,  0.5076, -0.9022,  0.6835,  0.2868,\n",
      "        -0.4572, -0.4660, -0.5974,  1.4851, -0.0067, -0.0824, -0.0413,  0.3138,\n",
      "         0.5998, -0.5274, -0.4722,  0.6480, -0.1594,  0.2582, -0.0886, -0.8017,\n",
      "        -0.1746, -0.9513, -0.2380,  0.3348], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    #생성자\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(input_dim,output_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        y = self.linear(x)\n",
    "\n",
    "        return y\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "model = NeuralNetwork(4,100)\n",
    "\n",
    "y = model.forward(x)\n",
    "\n",
    "print(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95839204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1,requires_grad=True) # 방법1\n",
    "\n",
    "y = torch.rand(1)\n",
    "\n",
    "y.requires_grad = True\n",
    "\n",
    "loss = y-x\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(x.grad,y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c16d3090",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(4)\n",
    "\n",
    "y = torch.zeros(3) # 실제값\n",
    "\n",
    "W = torch.rand(4,3,requires_grad = True)\n",
    "b = torch.rand(3,requires_grad = True)\n",
    "\n",
    "z = torch.matmul(x,W) + b   # 예측값\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cde64d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.0715, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.5398, 1.5029, 1.8616],\n",
      "        [1.5398, 1.5029, 1.8616],\n",
      "        [1.5398, 1.5029, 1.8616],\n",
      "        [1.5398, 1.5029, 1.8616]])\n",
      "tensor([1.5398, 1.5029, 1.8616])\n"
     ]
    }
   ],
   "source": [
    "# mse cost function\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 예측값과 실제값의 오차 계산\n",
    "loss = F.mse_loss(z,y)\n",
    "\n",
    "# 역전파\n",
    "loss.backward()\n",
    "\n",
    "print(loss)\n",
    "\n",
    "# W에 대한 미분값 예: 오차에 W가 얼마나 영향을 줬나... loss 의 미분값인데 W에 대한 미분값 \n",
    "# y^ = wx+b 라고한다면 오차제곱을 미분한뒤 w의 영향력 즉 x가 w에 직접적인 영향을 주기때문에 x를 곱해준다\n",
    "# 그렇다면 이런식 W = 2(y^ - y) * x \n",
    "print(W.grad)\n",
    "\n",
    "# b 가중치에 대한 미분값 이미 y햇 값에는 b값이 반영되어있기때문에 그냥 loss 미분 공식만 필요함 2(y^-y)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b391cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.5070,  0.2314,  0.7625],\n",
       "         [ 0.4071,  0.5256,  0.5685],\n",
       "         [ 0.4177,  0.8318, -0.1426],\n",
       "         [ 0.1336, -0.1273,  0.4124]], grad_fn=<SubBackward0>),\n",
       " tensor([0.0744, 0.0414, 0.2608], grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "W = W - learning_rate * W.grad\n",
    "b = b - learning_rate * b.grad\n",
    "\n",
    "W,b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8571de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73a9d424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\main\\AppData\\Local\\Temp\\ipykernel_5480\\2112647343.py:6: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  W = W - learning_rate * W.grad\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m learning_rate = \u001b[32m0.1\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m iterration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m,\u001b[32m31\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     W = W - \u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\n\u001b[32m      7\u001b[39m     b = b - learning_rate * b.grad\n\u001b[32m      9\u001b[39m     W.detach_().requires_grad(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "for iterration in range(1,31):\n",
    "\n",
    "\n",
    "    W = W - learning_rate * W.grad\n",
    "    b = b - learning_rate * b.grad\n",
    "\n",
    "    W.detach_().requires_grad(True)\n",
    "    b.detach_().requires_grad(True)\n",
    "\n",
    "    z = torch.matmul(x,W) + b\n",
    "\n",
    "    loss = F.mse_loss(z,y)\n",
    "\n",
    "    loss.backward()\n",
    "    print(f'{iterration}/30 {loss}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d9914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 300 epoch / loss : 5.326570\n",
      "10 / 300 epoch / loss : 0.001602\n",
      "20 / 300 epoch / loss : 0.000000\n",
      "30 / 300 epoch / loss : 0.000000\n",
      "40 / 300 epoch / loss : 0.000000\n",
      "50 / 300 epoch / loss : 0.000000\n",
      "60 / 300 epoch / loss : 0.000000\n",
      "70 / 300 epoch / loss : 0.000000\n",
      "80 / 300 epoch / loss : 0.000000\n",
      "90 / 300 epoch / loss : 0.000000\n",
      "100 / 300 epoch / loss : 0.000000\n",
      "110 / 300 epoch / loss : 0.000000\n",
      "120 / 300 epoch / loss : 0.000000\n",
      "130 / 300 epoch / loss : 0.000000\n",
      "140 / 300 epoch / loss : 0.000000\n",
      "150 / 300 epoch / loss : 0.000000\n",
      "160 / 300 epoch / loss : 0.000000\n",
      "170 / 300 epoch / loss : 0.000000\n",
      "180 / 300 epoch / loss : 0.000000\n",
      "190 / 300 epoch / loss : 0.000000\n",
      "200 / 300 epoch / loss : 0.000000\n",
      "210 / 300 epoch / loss : 0.000000\n",
      "220 / 300 epoch / loss : 0.000000\n",
      "230 / 300 epoch / loss : 0.000000\n",
      "240 / 300 epoch / loss : 0.000000\n",
      "250 / 300 epoch / loss : 0.000000\n",
      "260 / 300 epoch / loss : 0.000000\n",
      "270 / 300 epoch / loss : 0.000000\n",
      "280 / 300 epoch / loss : 0.000000\n",
      "290 / 300 epoch / loss : 0.000000\n",
      "300 / 300 epoch / loss : 0.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 입력변수\n",
    "x = torch.ones(4)\n",
    "\n",
    "# 목표변수\n",
    "y = torch.zeros(3) # 실제값\n",
    "\n",
    "# 가중치\n",
    "W = torch.rand(4,3,requires_grad = True)\n",
    "\n",
    "# 바이어스\n",
    "b = torch.rand(3,requires_grad = True)\n",
    "\n",
    "# 예측값\n",
    "z = torch.matmul(x,W) + b   # 예측값\n",
    "\n",
    "optimizer = torch.optim.SGD([W,b],lr = learning_rate)\n",
    "\n",
    "num_epoch = 300 \n",
    "\n",
    "for epoch in range(num_epoch + 1):\n",
    "    \n",
    "\n",
    "    z = torch.matmul(x,W) + b\n",
    "\n",
    "    loss = F.mse_loss(z,y)\n",
    "\n",
    "    optimizer.zero_grad() # grad를 초기화 하지 않으면 이전 gradient 가 계속 반영되어서 잘못되기때문\n",
    "    \n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'{epoch} / {num_epoch} epoch / loss : {loss.item():.6f}')\n",
    "        # print(f'loss : {loss.item():.6f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55f372b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinnearModel(\n",
      "  (linear): Linear(in_features=4, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinnearModel(nn.Module):\n",
    "\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(input_dim,output_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "\n",
    "input_dim =4\n",
    "\n",
    "output_dim = 3\n",
    "\n",
    "model = LinnearModel(input_dim,output_dim)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e076a54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 300 epoch / loss : 1.194636\n",
      "10 / 300 epoch / loss : 0.000359\n",
      "20 / 300 epoch / loss : 0.000000\n",
      "30 / 300 epoch / loss : 0.000000\n",
      "40 / 300 epoch / loss : 0.000000\n",
      "50 / 300 epoch / loss : 0.000000\n",
      "60 / 300 epoch / loss : 0.000000\n",
      "70 / 300 epoch / loss : 0.000000\n",
      "80 / 300 epoch / loss : 0.000000\n",
      "90 / 300 epoch / loss : 0.000000\n",
      "100 / 300 epoch / loss : 0.000000\n",
      "110 / 300 epoch / loss : 0.000000\n",
      "120 / 300 epoch / loss : 0.000000\n",
      "130 / 300 epoch / loss : 0.000000\n",
      "140 / 300 epoch / loss : 0.000000\n",
      "150 / 300 epoch / loss : 0.000000\n",
      "160 / 300 epoch / loss : 0.000000\n",
      "170 / 300 epoch / loss : 0.000000\n",
      "180 / 300 epoch / loss : 0.000000\n",
      "190 / 300 epoch / loss : 0.000000\n",
      "200 / 300 epoch / loss : 0.000000\n",
      "210 / 300 epoch / loss : 0.000000\n",
      "220 / 300 epoch / loss : 0.000000\n",
      "230 / 300 epoch / loss : 0.000000\n",
      "240 / 300 epoch / loss : 0.000000\n",
      "250 / 300 epoch / loss : 0.000000\n",
      "260 / 300 epoch / loss : 0.000000\n",
      "270 / 300 epoch / loss : 0.000000\n",
      "280 / 300 epoch / loss : 0.000000\n",
      "290 / 300 epoch / loss : 0.000000\n",
      "300 / 300 epoch / loss : 0.000000\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(4)\n",
    "\n",
    "y = torch.zeros(3)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epoch = 300 \n",
    "\n",
    "for epoch in range(num_epoch + 1):\n",
    "    \n",
    "\n",
    "    predict = model(x)\n",
    "    # 오차 계산 \n",
    "    loss = F.mse_loss(predict,y)\n",
    "\n",
    "    optimizer.zero_grad() # grad를 초기화 하지 않으면 이전 gradient 가 계속 반영되어서 잘못되기때문\n",
    "    \n",
    "    # 기울기 계산\n",
    "    loss.backward() \n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'{epoch} / {num_epoch} epoch / loss : {loss.item():.6f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb120c4",
   "metadata": {},
   "source": [
    "## activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64f2b9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# activation function 이 포함된 리니어모델\n",
    "class LineaRegression(nn.Module):\n",
    "\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "\n",
    "        super().__init__()\n",
    "        # 모델\n",
    "        self.linear = nn.Linear(input_dim,output_dim)\n",
    "        # 활성 함수\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 리니어 x에서 받은 결과값을 activation 함수에 전달하겠다.\n",
    "        return self.activation(self.linear(x))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72c088d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(4)\n",
    "y = torch.zeros(3)\n",
    "\n",
    "model = LineaRegression(4,3)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7613f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 100 epoch | loss : 0.000004\n",
      "1 / 100 epoch | loss : 0.000002\n",
      "2 / 100 epoch | loss : 0.000001\n",
      "3 / 100 epoch | loss : 0.000000\n",
      "4 / 100 epoch | loss : 0.000000\n",
      "5 / 100 epoch | loss : 0.000733\n",
      "6 / 100 epoch | loss : 0.000002\n",
      "7 / 100 epoch | loss : 0.000000\n",
      "8 / 100 epoch | loss : 0.000000\n",
      "9 / 100 epoch | loss : 0.000000\n",
      "10 / 100 epoch | loss : 0.000001\n",
      "11 / 100 epoch | loss : 0.000001\n",
      "12 / 100 epoch | loss : 0.000001\n",
      "13 / 100 epoch | loss : 0.000001\n",
      "14 / 100 epoch | loss : 0.000002\n",
      "15 / 100 epoch | loss : 0.000002\n",
      "16 / 100 epoch | loss : 0.000002\n",
      "17 / 100 epoch | loss : 0.000002\n",
      "18 / 100 epoch | loss : 0.000003\n",
      "19 / 100 epoch | loss : 0.000003\n",
      "20 / 100 epoch | loss : 0.000003\n",
      "21 / 100 epoch | loss : 0.000003\n",
      "22 / 100 epoch | loss : 0.000003\n",
      "23 / 100 epoch | loss : 0.000004\n",
      "24 / 100 epoch | loss : 0.000004\n",
      "25 / 100 epoch | loss : 0.000004\n",
      "26 / 100 epoch | loss : 0.000004\n",
      "27 / 100 epoch | loss : 0.000004\n",
      "28 / 100 epoch | loss : 0.000004\n",
      "29 / 100 epoch | loss : 0.000004\n",
      "30 / 100 epoch | loss : 0.000004\n",
      "31 / 100 epoch | loss : 0.000005\n",
      "32 / 100 epoch | loss : 0.000005\n",
      "33 / 100 epoch | loss : 0.000005\n",
      "34 / 100 epoch | loss : 0.000005\n",
      "35 / 100 epoch | loss : 0.000005\n",
      "36 / 100 epoch | loss : 0.000005\n",
      "37 / 100 epoch | loss : 0.000005\n",
      "38 / 100 epoch | loss : 0.000005\n",
      "39 / 100 epoch | loss : 0.000005\n",
      "40 / 100 epoch | loss : 0.000005\n",
      "41 / 100 epoch | loss : 0.000005\n",
      "42 / 100 epoch | loss : 0.000005\n",
      "43 / 100 epoch | loss : 0.000005\n",
      "44 / 100 epoch | loss : 0.000005\n",
      "45 / 100 epoch | loss : 0.000005\n",
      "46 / 100 epoch | loss : 0.000005\n",
      "47 / 100 epoch | loss : 0.000005\n",
      "48 / 100 epoch | loss : 0.000005\n",
      "49 / 100 epoch | loss : 0.000005\n",
      "50 / 100 epoch | loss : 0.000005\n",
      "51 / 100 epoch | loss : 0.000005\n",
      "52 / 100 epoch | loss : 0.000005\n",
      "53 / 100 epoch | loss : 0.000005\n",
      "54 / 100 epoch | loss : 0.000005\n",
      "55 / 100 epoch | loss : 0.000005\n",
      "56 / 100 epoch | loss : 0.000005\n",
      "57 / 100 epoch | loss : 0.000005\n",
      "58 / 100 epoch | loss : 0.000005\n",
      "59 / 100 epoch | loss : 0.000005\n",
      "60 / 100 epoch | loss : 0.000005\n",
      "61 / 100 epoch | loss : 0.000005\n",
      "62 / 100 epoch | loss : 0.000005\n",
      "63 / 100 epoch | loss : 0.000005\n",
      "64 / 100 epoch | loss : 0.000005\n",
      "65 / 100 epoch | loss : 0.000005\n",
      "66 / 100 epoch | loss : 0.000005\n",
      "67 / 100 epoch | loss : 0.000005\n",
      "68 / 100 epoch | loss : 0.000005\n",
      "69 / 100 epoch | loss : 0.000005\n",
      "70 / 100 epoch | loss : 0.000005\n",
      "71 / 100 epoch | loss : 0.000005\n",
      "72 / 100 epoch | loss : 0.000005\n",
      "73 / 100 epoch | loss : 0.000005\n",
      "74 / 100 epoch | loss : 0.000005\n",
      "75 / 100 epoch | loss : 0.000005\n",
      "76 / 100 epoch | loss : 0.000005\n",
      "77 / 100 epoch | loss : 0.000005\n",
      "78 / 100 epoch | loss : 0.000005\n",
      "79 / 100 epoch | loss : 0.000005\n",
      "80 / 100 epoch | loss : 0.000005\n",
      "81 / 100 epoch | loss : 0.000005\n",
      "82 / 100 epoch | loss : 0.000005\n",
      "83 / 100 epoch | loss : 0.000005\n",
      "84 / 100 epoch | loss : 0.000005\n",
      "85 / 100 epoch | loss : 0.000005\n",
      "86 / 100 epoch | loss : 0.000005\n",
      "87 / 100 epoch | loss : 0.000005\n",
      "88 / 100 epoch | loss : 0.000005\n",
      "89 / 100 epoch | loss : 0.000005\n",
      "90 / 100 epoch | loss : 0.000005\n",
      "91 / 100 epoch | loss : 0.000005\n",
      "92 / 100 epoch | loss : 0.000005\n",
      "93 / 100 epoch | loss : 0.000005\n",
      "94 / 100 epoch | loss : 0.000005\n",
      "95 / 100 epoch | loss : 0.000005\n",
      "96 / 100 epoch | loss : 0.000005\n",
      "97 / 100 epoch | loss : 0.000005\n",
      "98 / 100 epoch | loss : 0.000005\n",
      "99 / 100 epoch | loss : 0.000005\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 학습속도\n",
    "learning_rate = 0.01\n",
    "# 몇 에폭 돌릴건지\n",
    "nb_epochs = 100\n",
    "\n",
    "# 옵티마이져\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "\n",
    "    pred = model(x)\n",
    "    # 예측값 실제값 계산\n",
    "    loss =loss_function(pred,y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    print(f'{epoch} / {nb_epochs} epoch | loss : {loss.item():.6f}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch GPU (torch_py310)",
   "language": "python",
   "name": "torch_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
